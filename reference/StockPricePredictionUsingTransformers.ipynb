{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AJOLtEZ04sYU"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, GlobalAveragePooling1D\n",
        "\n",
        "import yfinance as yf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bollinger_bands(data, window=10, num_of_std=2):\n",
        "    \"\"\"Calculate Bollinger Bands\"\"\"\n",
        "    rolling_mean = data.rolling(window=window).mean()\n",
        "    rolling_std = data.rolling(window=window).std()\n",
        "    upper_band = rolling_mean + (rolling_std * num_of_std)\n",
        "    lower_band = rolling_mean - (rolling_std * num_of_std)\n",
        "    return upper_band, lower_band\n",
        "\n",
        "def calculate_rsi(data, window=10):\n",
        "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
        "    delta = data.diff()\n",
        "    gain = delta.clip(lower=0)\n",
        "    loss = -delta.clip(upper=0)\n",
        "    avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
        "    avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "def calculate_roc(data, periods=10):\n",
        "    \"\"\"Calculate Rate of Change.\"\"\"\n",
        "    roc = ((data - data.shift(periods)) / data.shift(periods)) * 100\n",
        "    return roc"
      ],
      "metadata": {
        "id": "ziguS6k842Uo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tickers = ['META', 'AAPL', 'MSFT', 'AMZN', 'GOOG']"
      ],
      "metadata": {
        "id": "RS9sQ0rD6RkC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ticker_data_frames = []\n",
        "stats = {}\n",
        "for ticker in tickers:\n",
        "\n",
        "    # Download historical data for the ticker\n",
        "    data = yf.download(ticker, period=\"1mo\", interval=\"5m\")\n",
        "\n",
        "    # Calculate the daily percentage change\n",
        "    close = data['Close']\n",
        "    upper, lower = calculate_bollinger_bands(close, window=14, num_of_std=2)\n",
        "    width = upper - lower\n",
        "    rsi = calculate_rsi(close, window=14)\n",
        "    roc = calculate_roc(close, periods=14)\n",
        "    volume = data['Volume']\n",
        "    diff = data['Close'].diff(1)\n",
        "    percent_change_close = data['Close'].pct_change() * 100\n",
        "\n",
        "    # Create a DataFrame for the current ticker and append it to the list\n",
        "    ticker_df = pd.DataFrame({\n",
        "        ticker+'_close': close,\n",
        "        ticker+'_width': width,\n",
        "        ticker+'_rsi': rsi,\n",
        "        ticker+'_roc': roc,\n",
        "        ticker+'_volume': volume,\n",
        "        ticker+'_diff': diff,\n",
        "        ticker+'_percent_change_close': percent_change_close,\n",
        "    }, index=close.index)\n",
        "\n",
        "    MEAN = ticker_df.mean()\n",
        "    STD = ticker_df.std()\n",
        "\n",
        "    # Keep track of mean and std\n",
        "    for column in MEAN.index:\n",
        "      stats[f\"{column}_mean\"] = MEAN[column]\n",
        "      stats[f\"{column}_std\"] = STD[column]\n",
        "\n",
        "    # Normalize the training features\n",
        "    ticker_df = (ticker_df - MEAN) / STD\n",
        "\n",
        "    ticker_data_frames.append(ticker_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "MxDXk9uu6VBI",
        "outputId": "a4f448ae-47c1-4fdd-8636-e706877b44f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Data must be 1-dimensional, got ndarray of shape (1484, 1) instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4a1dd09bb492>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Create a DataFrame for the current ticker and append it to the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     ticker_df = pd.DataFrame({\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mticker\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_close'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mticker\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_width'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# _homogenize ensures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m#  - all(len(x) == len(index) for x in arrays)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mrefs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         return sanitize_array(\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m     \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_ndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36m_sanitize_ndim\u001b[0;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    719\u001b[0m                 \u001b[0;34mf\"Data must be 1-dimensional, got ndarray of shape {data.shape} instead\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Data must be 1-dimensional, got ndarray of shape (1484, 1) instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert stats from dict to df\n",
        "stats = pd.DataFrame([stats], index=[0])\n",
        "stats.head()"
      ],
      "metadata": {
        "id": "PoLOIDNN6jFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all ticker DataFrames\n",
        "df = pd.concat(ticker_data_frames, axis=1)\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qekxvEZk7woI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shift the df data to create labels\n",
        "labels = df.shift(-1)\n",
        "\n",
        "# Drop the last row in both percent_change_data and labels as it won't have a corresponding label\n",
        "df = df.iloc[:-1]\n",
        "labels = labels.iloc[:-1]"
      ],
      "metadata": {
        "id": "l_DDzDQp8Hju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence len = 24 means that we have 2 hours of 5 min data\n",
        "SEQUENCE_LEN = 24\n",
        "\n",
        "# Function to create X-day sequences for each ticker\n",
        "def create_sequences(data, labels, mean, std, sequence_length=SEQUENCE_LEN):\n",
        "    sequences = []\n",
        "    lab = []\n",
        "    data_size = len(data)\n",
        "\n",
        "    # + 12 because we want to predict the next hour\n",
        "    for i in range(data_size - (sequence_length + 13)):\n",
        "      if i == 0:\n",
        "        continue\n",
        "\n",
        "      sequences.append(data[i:i + sequence_length])\n",
        "      lab.append([labels[i-1], labels[i + 12], mean[0], std[0]])\n",
        "\n",
        "    for i in range(0, len(lab)):\n",
        "      last_price_data = sequences[i][-1][0]\n",
        "      last_price_label = lab[i][0]\n",
        "\n",
        "      if not last_price_data == last_price_label:\n",
        "        print(f\"ERROR : {last_price_data=} and {last_price_label=} are not equal\")\n",
        "\n",
        "    return np.array(sequences), np.array(lab)"
      ],
      "metadata": {
        "id": "k0Jdnzly9FQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_dict = {}\n",
        "sequence_labels = {}\n",
        "for ticker in tickers:\n",
        "\n",
        "    # Extract close and volume data for the ticker\n",
        "    close = df[ticker+'_close'].values\n",
        "    width = df[ticker+'_width'].values\n",
        "    rsi = df[ticker+'_rsi'].values\n",
        "    roc = df[ticker+'_roc'].values\n",
        "    volume = df[ticker+'_volume'].values\n",
        "    diff = df[ticker+'_diff'].values\n",
        "    pct_change = df[ticker+'_percent_change_close'].values\n",
        "\n",
        "    # Combine close and volume data\n",
        "    ticker_data = np.column_stack((close,\n",
        "                                   width,\n",
        "                                   rsi,\n",
        "                                   roc,\n",
        "                                   volume,\n",
        "                                   diff,\n",
        "                                   pct_change))\n",
        "\n",
        "    # Generate sequences\n",
        "    attribute = ticker+\"_close\"\n",
        "    ticker_sequences, lab = create_sequences(ticker_data,\n",
        "                                             labels[attribute].values[SEQUENCE_LEN-1:],\n",
        "                                             stats[attribute+\"_mean\"].values,\n",
        "                                             stats[attribute+\"_std\"].values)\n",
        "\n",
        "    sequences_dict[ticker] = ticker_sequences\n",
        "    sequence_labels[ticker] = lab"
      ],
      "metadata": {
        "id": "y4Mh2yj79Ud9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine data and labels from all tickers\n",
        "all_sequences = []\n",
        "all_labels = []\n",
        "\n",
        "for ticker in tickers:\n",
        "    all_sequences.extend(sequences_dict[ticker])\n",
        "    all_labels.extend(sequence_labels[ticker])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_sequences = np.array(all_sequences)\n",
        "all_labels = np.array(all_labels)"
      ],
      "metadata": {
        "id": "-oXY-sXP-fnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "shuffled_indices = np.random.permutation(len(all_sequences))\n",
        "all_sequences = all_sequences[shuffled_indices]\n",
        "all_labels = all_labels[shuffled_indices]\n",
        "\n",
        "train_size = int(len(all_sequences) * 0.9)\n",
        "\n",
        "# Split sequences\n",
        "train_sequences = all_sequences[:train_size]\n",
        "train_labels    = all_labels[:train_size]\n",
        "\n",
        "other_sequences = all_sequences[train_size:]\n",
        "other_labels    = all_labels[train_size:]\n",
        "\n",
        "shuffled_indices = np.random.permutation(len(other_sequences))\n",
        "other_sequences = other_sequences[shuffled_indices]\n",
        "other_labels = other_labels[shuffled_indices]\n",
        "\n",
        "val_size = int(len(other_sequences) * 0.5)\n",
        "\n",
        "validation_sequences = other_sequences[:val_size]\n",
        "validation_labels = other_labels[:val_size]\n",
        "\n",
        "test_sequences = other_sequences[val_size:]\n",
        "test_labels = other_labels[val_size:]"
      ],
      "metadata": {
        "id": "8QY42PaV-kPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Attention and Normalization\n",
        "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
        "    x = Add()([x, inputs])\n",
        "\n",
        "    # Feed Forward Part\n",
        "    y = LayerNormalization(epsilon=1e-6)(x)\n",
        "    y = Dense(ff_dim, activation=\"relu\")(y)\n",
        "    y = Dropout(dropout)(y)\n",
        "    y = Dense(inputs.shape[-1])(y)\n",
        "    return Add()([y, x])\n",
        "\n",
        "def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout=0):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "\n",
        "    # Create multiple layers of the Transformer block\n",
        "    for _ in range(num_layers):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    # Final part of the model\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    outputs = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "    # Compile model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Model parameters\n",
        "input_shape = train_sequences.shape[1:]\n",
        "head_size = 256\n",
        "num_heads = 16\n",
        "ff_dim = 1024\n",
        "num_layers = 12\n",
        "dropout = 0.20\n",
        "\n",
        "# Build the model\n",
        "model = build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kBKt-Fhj-p3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_mae_loss(y_true, y_pred):\n",
        "    y_true_next = tf.cast(y_true[:, 1], tf.float64)\n",
        "    y_pred_next = tf.cast(y_pred[:, 0], tf.float64)\n",
        "    abs_error = tf.abs(y_true_next - y_pred_next)\n",
        "\n",
        "    return tf.reduce_mean(abs_error)\n",
        "\n",
        "def dir_acc(y_true, y_pred):\n",
        "    mean, std = tf.cast(y_true[:, 2], tf.float64), tf.cast(y_true[:, 3], tf.float64)\n",
        "\n",
        "    y_true_prev = (tf.cast(y_true[:, 0], tf.float64) * std) + mean\n",
        "    y_true_next = (tf.cast(y_true[:, 1], tf.float64) * std) + mean\n",
        "    y_pred_next = (tf.cast(y_pred[:, 0], tf.float64) * std) + mean\n",
        "\n",
        "    true_change = y_true_next - y_true_prev\n",
        "    pred_change = y_pred_next - y_true_prev\n",
        "\n",
        "    correct_direction = tf.equal(tf.sign(true_change), tf.sign(pred_change))\n",
        "\n",
        "    return tf.reduce_mean(tf.cast(correct_direction, tf.float64))"
      ],
      "metadata": {
        "id": "bWMOzku9-vpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss=custom_mae_loss, metrics=[dir_acc])"
      ],
      "metadata": {
        "id": "CQ20qXHdABEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a callback to save the best model\n",
        "checkpoint_callback_train = ModelCheckpoint(\n",
        "    \"transformer_train_model.keras\",  # Filepath to save the best model\n",
        "    monitor=\"dir_acc\",  #\"loss\",  # Metric to monitor\n",
        "    save_best_only=True,  # Save only the best model\n",
        "    mode=\"max\",  # Minimize the monitored metric\n",
        "    verbose=1,  # Display progress\n",
        ")\n",
        "\n",
        "# Define a callback to save the best model\n",
        "checkpoint_callback_val = ModelCheckpoint(\n",
        "    \"transformer_val_model.keras\",  # Filepath to save the best model\n",
        "    monitor=\"val_dir_acc\", #\"val_loss\",  # Metric to monitor\n",
        "    save_best_only=True,  # Save only the best model\n",
        "    mode=\"max\",  # Minimize the monitored metric\n",
        "    verbose=1,  # Display progress\n",
        ")\n",
        "\n",
        "def get_lr_callback(batch_size=16, mode='cos', epochs=500, plot=False):\n",
        "    lr_start, lr_max, lr_min = 0.0001, 0.005, 0.00001  # Adjust learning rate boundaries\n",
        "    lr_ramp_ep = int(0.30 * epochs)  # 30% of epochs for warm-up\n",
        "    lr_sus_ep = max(0, int(0.10 * epochs) - lr_ramp_ep)  # Optional sustain phase, adjust as needed\n",
        "\n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_ramp_ep:  # Warm-up phase\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:  # Sustain phase at max learning rate\n",
        "            lr = lr_max\n",
        "        elif mode == 'cos':\n",
        "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep, epoch - lr_ramp_ep - lr_sus_ep\n",
        "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
        "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
        "        else:\n",
        "            lr = lr_min  # Default to minimum learning rate if mode is not recognized\n",
        "\n",
        "        return lr\n",
        "\n",
        "    if plot:  # Plot learning rate curve if plot is True\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Learning Rate')\n",
        "        plt.title('Learning Rate Scheduler')\n",
        "        plt.show()\n",
        "\n",
        "    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)"
      ],
      "metadata": {
        "id": "1kVujFs0BQu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "model.fit(train_sequences, train_labels,\n",
        "          validation_data=(validation_sequences, validation_labels),\n",
        "          epochs=EPOCHS,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          shuffle=True,\n",
        "          callbacks=[checkpoint_callback_train, checkpoint_callback_val, get_lr_callback(batch_size=BATCH_SIZE, epochs=EPOCHS)])"
      ],
      "metadata": {
        "id": "4Mv3B5gvBeR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Weights\n",
        "model.load_weights(\"transformer_val_model.keras\")\n",
        "\n",
        "# Make predictions\n",
        "accuracy = model.evaluate(test_sequences, test_labels)[1]\n",
        "print(accuracy)\n",
        "\n",
        "# Calculate additional metrics as needed\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "predictions = model.predict(test_sequences)\n",
        "r2 = r2_score(test_labels[:, 1], predictions[:, 0])\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "id": "gFpSqzgXCE4K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}